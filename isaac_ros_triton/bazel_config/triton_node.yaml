%YAML 1.2
# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES
# Copyright (c) 2022-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0
---
name: triton_server
components:
- name: server
  type: nvidia::triton::TritonServer
  parameters:
    log_level: 2
    model_repository_paths: [model_repository_path_placeholder]
    backend_directory_path: ./ros_ws/src/isaac_ros_dnn_inference/isaac_ros_triton/ros_ws/opt/tritonserver/backends/
---
name: triton_request
components:
- name: input
  type: nvidia::gxf::DoubleBufferReceiver
  parameters:
    capacity: 1
- type: nvidia::gxf::MessageAvailableSchedulingTerm
  parameters:
    receiver: input
    min_size: 1
- name: inferencer_impl
  type: nvidia::triton::TritonInferencerImpl
  parameters:
    server: triton_server/server
    model_name: model_name_placeholder
    model_version: 1
    num_concurrent_requests: 10
    max_batch_size: 0
    async_scheduling_term: triton_response/async_st
    inference_mode: Direct
    output_storage_type: 0
    use_sequence_data: true
- name: requester
  type: nvidia::triton::TritonInferenceRequest
  parameters:
    inferencer: inferencer_impl
    dummy_rx: input
    rx: [input]
    input_tensor_names: [input_tensor_name_placeholder]
    input_binding_names: [input_binding_name_placeholder]
- name: triton_request_receptive_scheduling_term
  type: nvidia::triton::TritonRequestReceptiveSchedulingTerm
  parameters:
    inferencer: inferencer_impl
---
name: triton_response
components:
- name: output
  type: nvidia::gxf::DoubleBufferTransmitter
  parameters:
    capacity: 1
- type: nvidia::gxf::DownstreamReceptiveSchedulingTerm
  parameters:
    transmitter: output
    min_size: 1
- type: nvidia::triton::TritonInferenceResponse
  parameters:
    inferencer: triton_request/inferencer_impl
    tx: output
    output_tensor_names: [output_tensor_name_placeholder]
    output_binding_names: [output_binding_name_placeholder]
- name: async_st
  type: nvidia::gxf::AsynchronousSchedulingTerm
---
name: tensor_copier
components:
- name: input
  type: nvidia::gxf::DoubleBufferReceiver
  parameters:
    capacity: 1
- type: nvidia::gxf::MessageAvailableSchedulingTerm
  parameters:
    receiver: input
    min_size: 1
- name: output
  type: nvidia::gxf::DoubleBufferTransmitter
  parameters:
    capacity: 1
- type: nvidia::gxf::DownstreamReceptiveSchedulingTerm
  parameters:
    transmitter: output
    min_size: 1
- name: allocator
  type: nvidia::gxf::UnboundedAllocator
- type: nvidia::gxf::TensorCopier
  parameters:
    receiver: input
    transmitter: output
    allocator: allocator
    mode: 0
---
name: sink
components:
- name: signal
  type: nvidia::gxf::DoubleBufferReceiver
  parameters:
    capacity: 1
- type: nvidia::gxf::MessageAvailableSchedulingTerm
  parameters:
    receiver: signal
    min_size: 1
- name: sink
  type: nvidia::isaac_ros::MessageRelay
  parameters:
    source: signal
---
components:
- type: nvidia::gxf::Connection
  parameters:
    source: triton_response/output
    target: tensor_copier/input
- type: nvidia::gxf::Connection
  parameters:
    source: tensor_copier/output
    target: sink/signal
---
components:
- type: nvidia::gxf::GreedyScheduler
  parameters:
    clock: clock
    stop_on_deadlock: false
    check_recession_period_ms: 0.1
- name: clock
  type: nvidia::gxf::RealtimeClock
