// SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES
// Copyright (c) 2020-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
// SPDX-License-Identifier: Apache-2.0

syntax = "proto3";
package nvdsinferserver.config;

enum MediaFormat {
  MEDIA_FORMAT_NONE = 0;
  IMAGE_FORMAT_RGB = 1;
  IMAGE_FORMAT_BGR = 2;
  IMAGE_FORMAT_GRAY = 3;
}

enum TensorOrder {
  TENSOR_ORDER_NONE = 0;
  TENSOR_ORDER_LINEAR = 1;
  TENSOR_ORDER_NHWC = 2;
}

enum TensorDataType {
  TENSOR_DT_NONE = 0;
  TENSOR_DT_FP32 = 1;
  TENSOR_DT_FP16 = 2;
  TENSOR_DT_INT8 = 3;
  TENSOR_DT_INT16 = 4;
  TENSOR_DT_INT32 = 5;
  TENSOR_DT_UINT8 = 6;
  TENSOR_DT_UINT16 = 7;
  TENSOR_DT_UINT32 = 8;
  TENSOR_DT_FP64 = 9;
  TENSOR_DT_INT64 = 10;
  TENSOR_DT_UINT64 = 11;
  TENSOR_DT_STRING = 12;
}

enum FrameScalingHW {
  FRAME_SCALING_HW_DEFAULT = 0;
  FRAME_SCALING_HW_GPU = 1;
  FRAME_SCALING_HW_VIC = 2;
}

/** Tensor memory type
 */
enum MemoryType {
  MEMORY_TYPE_DEFAULT = 0;
  MEMORY_TYPE_CPU = 1;
  MEMORY_TYPE_GPU = 2;
}

/** Custom lib for preload */
message CustomLib {
  /** Path point to the custom library */
  string path = 1;
}

/** Network Input layer information */
message InputLayer {
  /** input tensor name, optional*/
  string name = 1;
  /** fixed inference shape, only required when backend has wildcard shape */
  repeated int32 dims = 2;
  /** tensor data type, optional. default TENSOR_DT_NONE */
  TensorDataType data_type = 3;
}

/** Network Onput layer information */
message OutputLayer {
  /** output tensor name */
  string name = 1;
  /** set max buffer bytes for output tensor */
  uint64 max_buffer_bytes = 2;
}


/** preprocessing settings */
message PreProcessParams {
  /** Input data normalization settings */
  message ScaleNormalize
  {
    /** Normalization factor to scale the input pixels with. */
    float scale_factor = 1;
    /** Per channel offsets for mean subtraction. This is an alternative to
     * the mean image file. The number of offsets in the array should be
     * exactly equalto the number of input channels.
     */
    repeated float channel_offsets = 2;
    /** Path to the mean image file (PPM format). Resolution of the file
     * should be equal to the network input resolution.
     */
    string mean_file = 3;
  }
  /** Network input format */
  MediaFormat network_format = 1;
  /** Network input tensor order */
  TensorOrder tensor_order = 2;
  /** preprocessing data set to network tensor name */
  string tensor_name = 3;
  /** Indicating if aspect ratio should be maintained when scaling to
   * network resolution. Right/bottom areas will be filled with black areas. */
  int32 maintain_aspect_ratio = 4;
  /** Compute hardware to use for scaling frames / objects. */
  FrameScalingHW frame_scaling_hw = 5;
  /** Interpolation filter to use while scaling. Refer to
   * NvBufSurfTransform_Inter for supported filter values. */
  uint32 frame_scaling_filter = 6;
  /** Preprocessing methods */
  oneof preprocess_method {
    /** usual scaling normalization for images */
    ScaleNormalize normalize = 7;
  }
  /** Indicating if symmetric padding should be used or not while scaling
  * to network resolution. Bottom-right padding is used by default. */
  int32 symmetric_padding = 8;
}

/** Deepstream Detection settings */
message DetectionParams {
  /** non-maximum-suppression cluster method */
  message Nms
  {
    /** detection score less this threshold would be rejected */
    float confidence_threshold = 1;
    /** IOU threshold */
    float iou_threshold = 2;
    /** top kth detection results to keep after nms. 0), keep all */
    int32 topk = 3;
  }

  /** DBScan object clustering */
  message DbScan {
    /** Bounding box detection threshold. */
    float pre_threshold = 1;
    // float post_threshold = 2;
    /** Epsilon to control merging of overlapping boxes */
    float eps = 3;
    /** Minimum boxes in a cluster to be considered an object */
    int32 min_boxes = 4;
    /** Minimum score in a cluster for it to be considered as an object */
    float min_score = 5;
  }

  /** cluster method based on grouping rectangles*/
  message GroupRectangle {
    /** detection score less this threshold would be rejected */
    float confidence_threshold = 1;
    /** how many bbox can be clustered together */
    int32 group_threshold = 2;
    /** Epsilon to control merging of overlapping boxes */
    float eps = 3;
  }

  /** simple cluster method for confidence filter */
  message SimpleCluster
  {
    /** detection score less this threshold would be rejected */
    float threshold = 1;
  }

  /** specific parameters controled per class*/
  message PerClassParams {
    /** pre-threshold used for filter out confidence less than the value */
    float pre_threshold = 1;
  }

  /** Number of classes detected by a detector network. */
  int32 num_detected_classes = 1;
  /** Per class detection parameters. key-value is for
   * <class_id:class_parameter> */
  map<int32, PerClassParams> per_class_params = 2;
  /** Name of the custom bounding box function in the custom library. */
  string custom_parse_bbox_func = 3;

  /** cluster methods for bbox, choose one only */
  oneof clustering_policy {
    /** non-maximum-suppression, reserved, not supported yet */
    Nms nms = 4;
    /** DbScan clustering parameters */
    DbScan dbscan = 5;
    /** grouping rectagules */
    GroupRectangle group_rectangle = 6;
    /** simple threshold filter */
    SimpleCluster simple_cluster = 7;
  }
}

/** Deepstream Classifciation settings */
message ClassificationParams {
  /** classifciation threshold */
  float threshold = 1;
  /** custom function for classification parsing */
  string custom_parse_classifier_func = 2;
}

/** Deepstream segmentation settings */
message SegmentationParams {
  /** reserved field */
  float threshold = 1;
}

/** Other Network settings, need application to do postprocessing */
message OtherNetworkParams {
  /** reserved field */
  string type_name = 1;
}

/** Triton classifcation settings */
message TritonClassifyParams
{
  /** top k classification results */
  uint32 topk = 1;
  /** classifciation threshold */
  float threshold = 2;
  /** [optional] specify which output tensor is used for triton classification.*/
  string tensor_name = 3;
}

/** Network LSTM Parameters */
message LstmParams {
  /** init constant value for lstm input tensors, usually zero or one */
  message InitConst {
    /** const value */
    float value = 1;
  }
  /** LSTM loop information */
  message LstmLoop {
    /** input tensor name */
    string input = 1;
    /** output tensor name */
    string output = 2;
    /** initialize input tensor for first frame */
    oneof init_state {
      /** init const value, default is zero */
      InitConst init_const = 3;
    }
    /** enable if need keep lstm output tensor data for application output
     * parsing, it's disabled by default */
    bool keep_output = 4;
  }
  repeated LstmLoop loops = 1;
}

