%YAML 1.2
# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES
# Copyright (c) 2021-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0
---
name: inference
components:
- name: tx
  type: nvidia::gxf::DoubleBufferTransmitter
  parameters:
    capacity: 1
- type: nvidia::gxf::DownstreamReceptiveSchedulingTerm
  parameters:
    transmitter: tx
    min_size: 1
- type: nvidia::gxf::MessageAvailableSchedulingTerm
  parameters:
    receiver: rx
    min_size: 1
- name: rx
  type: nvidia::gxf::DoubleBufferReceiver
  parameters:
    capacity: 1
- name: pool
  type: nvidia::gxf::BlockMemoryPool
  parameters:
    storage_type: 1
    block_size: 67108864
    num_blocks: 40
- type: nvidia::gxf::TensorRtInference
  parameters:
    model_file_path: /tmp/model_file.onnx
    engine_file_path: /tmp/trt_engine.plan
    input_tensor_names:
        - input
    input_binding_names:
        - data
    output_tensor_names:
        - output
    output_binding_names:
        - mobilenetv20_output_flatten0_reshape0
    pool: pool
    cuda_stream_pool: utils/stream
    rx: [rx]
    tx: tx
    force_engine_update: true
    verbose: true
    clock: utils/clock
- type: nvidia::gxf::MemoryAvailableSchedulingTerm
  parameters:
    allocator: pool
    min_blocks: 1
---
name: cuda_stream_sync
components:
- name: rx
  type: nvidia::gxf::DoubleBufferReceiver
  parameters:
    capacity: 1
- name: tx
  type: nvidia::gxf::DoubleBufferTransmitter
  parameters:
    capacity: 1
- type: nvidia::gxf::MessageAvailableSchedulingTerm
  parameters:
    receiver: rx
    min_size: 1
- type: nvidia::gxf::DownstreamReceptiveSchedulingTerm
  parameters:
    transmitter: tx
    min_size: 1
- type: nvidia::gxf::CudaStreamSync
  parameters:
    rx: rx
    tx: tx
---
name: sink
components:
- name: signal
  type: nvidia::gxf::DoubleBufferReceiver
  parameters:
    capacity: 1
- type: nvidia::gxf::MessageAvailableSchedulingTerm
  parameters:
    receiver: signal
    min_size: 1
- name: sink
  type: nvidia::isaac_ros::MessageRelay
  parameters:
    source: signal
---
name: connections
components:
- type: nvidia::gxf::Connection
  parameters:
    source: inference/tx
    target: cuda_stream_sync/rx
- type: nvidia::gxf::Connection
  parameters:
    source: cuda_stream_sync/tx
    target: sink/signal
---
name: utils
components:
- name: clock
  type: nvidia::gxf::RealtimeClock
- type: nvidia::gxf::MultiThreadScheduler
  parameters:
    clock: clock
    stop_on_deadlock: false
    check_recession_period_ms: 1
    worker_thread_number: 1
    worker_thread_name_id: "tensor_rt_inference"
- type: nvidia::gxf::JobStatistics
  parameters:
    clock: clock
- name: stream
  type: nvidia::gxf::CudaStreamPool
  parameters:
    dev_id: 0
    stream_flags: 1
    stream_priority: 0
    reserved_size: 1
    max_size: 5
    nvtx_identifier: "tensor_rt_inference_stream"
